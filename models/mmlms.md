# Multimodal Large Language Models (MMLMs) beyond VLMs
*Here're some resources about Multimodal Large Language Models (MMLMs) beyond VLMs*


### Multiple Modal


#### Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.08046)

github link: [here](https://github.com/PKU-YuanGroup/Chat-UniVi)

citation: 
```bibtex
@misc{jin2024chatunivi,
      title={Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding}, 
      author={Peng Jin and Ryuichi Takanobu and Wancai Zhang and Xiaochun Cao and Li Yuan},
      year={2024},
      eprint={2311.08046},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


### Video Modal


#### CAST: Cross-Attention in Space and Time for Video Action Recognition [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.18825.pdf)

citation: 
```bibtex
@misc{lee2023cast,
      title={CAST: Cross-Attention in Space and Time for Video Action Recognition}, 
      author={Dongho Lee and Jongseo Lee and Jinwoo Choi},
      year={2023},
      eprint={2311.18825},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Video-LLaVA: Learning United Visual Representation by Alignment Before Projection [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.10122)

github link: [here](https://github.com/PKU-YuanGroup/Video-LLaVA)


citation:
```bibtex
@article{lin2023video,
  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}
```

#### Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.14494)

citation: 
```bibtex
@article{huang2023free,
  title={Free-bloom: Zero-shot text-to-video generator with llm director and ldm animator},
  author={Huang, Hanzhuo and Feng, Yufan and Shi, Cheng and Xu, Lan and Yu, Jingyi and Yang, Sibei},
  journal={arXiv preprint arXiv:2309.14494},
  year={2023}
}
```


#### Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.05424)

github link: [here](https://github.com/mbzuai-oryx/Video-ChatGPT)

citation: 
```bibtex
@misc{maaz2023videochatgpt,
      title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models}, 
      author={Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan},
      year={2023},
      eprint={2306.05424},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```



### Audio Modal

#### Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.07919.pdf)

|model name|link|
|-|-|
|Qwen-Audio-Chat|[here](https://huggingface.co/Qwen/Qwen-Audio-Chat)|
|Qwen-Audio|[here](https://huggingface.co/Qwen/Qwen-Audio)|

citation:
```bibtex
@misc{chu2023qwenaudio,
      title={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models}, 
      author={Yunfei Chu and Jin Xu and Xiaohuan Zhou and Qian Yang and Shiliang Zhang and Zhijie Yan and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2311.07919},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}
```