# Visual Language Models (VLMs)
*Here're some resources about Visual Language Models (VLMs)*


#### MobileVLM V2: Faster and Stronger Baseline for Vision Language Model (LDPv2) [`READ`]

paper link: [here](https://arxiv.org/pdf/2402.03766)

github link: [here](ttps://github.com/Meituan-AutoML/MobileVLM)


citation:
```bibtex
@article{chu2024mobilevlm,
  title={MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}
```


#### MoE-LLaVA: Mixture of Experts for Large Vision-Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2401.15947)

github link: [here](https://github.com/PKU-YuanGroup/MoE-LLaVA)

model-zoo link: [here](https://github.com/PKU-YuanGroup/MoE-LLaVA?tab=readme-ov-file#-model-zoo)


citation:
```bibtex
@misc{lin2024moellava,
      title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models}, 
      author={Bin Lin and Zhenyu Tang and Yang Ye and Jiaxi Cui and Bin Zhu and Peng Jin and Jinfa Huang and Junwu Zhang and Munan Ning and Li Yuan},
      year={2024},
      eprint={2401.15947},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices (LDP) [`READ`]

paper link: [here](https://arxiv.org/pdf/2312.16886)

github link: [here](ttps://github.com/Meituan-AutoML/MobileVLM)

model links:

|model name|link|
|-|-|
|MobileLLaMA-1.4B-Chat|[here](https://huggingface.co/mtgv/MobileLLaMA-1.4B-Chat)|
|MobileLLaMA-1.4B-Base|[here](https://huggingface.co/mtgv/MobileLLaMA-1.4B-Base)|

updated version: [here](#mobilevlm-v2-faster-and-stronger-baseline-for-vision-language-model-ldpv2-read)

citation:
```bibtex
@article{chu2023mobilevlm,
  title={Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices},
  author={Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
  journal={arXiv preprint arXiv:2312.16886},
  year={2023}
}
```


#### ShareGPT4V: Improving Large Multi-Modal Models with Better Captions [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.03079.pdf)

github link: [here](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V)

homepage link: [here](https://sharegpt4v.github.io/)

dataset link: [here](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V)

model links:

|model name|link|
|-|-|
|ShareCaptioner|[here](https://huggingface.co/Lin-Chen/ShareCaptioner)|
|ShareGPT4V-7B|[here](https://huggingface.co/Lin-Chen/ShareGPT4V-7B)|


citation:
```bibtex
@misc{chen2023sharegpt4v,
      title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions}, 
      author={Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Conghui He and Jiaqi Wang and Feng Zhao and Dahua Lin},
      year={2023},
      eprint={2311.12793},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### CogVLM: Visual Expert for Pretrained Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2311.03079.pdf)

github link: [here](https://github.com/THUDM/CogVLM)

model links: 

|model name|link|
|-|-|
|CogVLM-chat-hf|[here](https://huggingface.co/THUDM/cogvlm-chat-hf)|
|CogVLM|[here](https://huggingface.co/THUDM/CogVLM)|

citation: 
```bibtex
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models}, 
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Improved Baselines with Visual Instruction Tuning (LLaVA-v1.5) [`READ`]

paper link: [here](https://arxiv.org/pdf/2310.03744.pdf)

blog link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)

model-zoo links: [v1.5](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) | [v1.6](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16)


citation: 
```bibtex
@misc{liu2023improvedllava,
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      title={Improved Baselines with Visual Instruction Tuning}, 
      publisher={arXiv:2310.03744},
      year={2023},
}
```


#### Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond [`READ`]

paper link: [here](https://arxiv.org/pdf/2308.12966.pdf)

github link: [here](https://github.com/QwenLM/Qwen-VL)

model links: 

|model name|link|
|-|-|
|Qwen-VL-Chat-Int4|[here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)|
|Qwen-VL-Chat|[here](https://huggingface.co/Qwen/Qwen-VL-Chat)|
|Qwen-VL|[here](https://huggingface.co/Qwen/Qwen-VL)|


citation: 
```bibtex
@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model (IDEFICS) [`READ`]

blog link: [here](https://huggingface.co/blog/idefics)

model links: 

|model name|link|
|-|-|
|idefics2-8b-chatty|[here](https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty)|
|idefics2-8b|[here](https://huggingface.co/HuggingFaceM4/idefics2-8b)|
|idefics2-8b-base|[here](https://huggingface.co/HuggingFaceM4/idefics2-8b-base)|
|idefics-80b-instruct|[here](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct)|
|idefics-80b|[here](https://huggingface.co/HuggingFaceM4/idefics-80b)|
|idefics-9b-instruct|[here](https://huggingface.co/HuggingFaceM4/idefics-9b-instruct)|
|idefics-9b|[here](https://huggingface.co/HuggingFaceM4/idefics-9b)|


citation: 
```bibtex
@misc{huggingface_blog_idefics,
  title = {Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Language Model},
  author = {Hugo Lauren√ßon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh},
  howpublished = {\url{https://huggingface.co/blog/idefics}},
  year = {2023},
  note = {Accessed: 2024-05-11}
}
```


#### OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2308.01390)

blog links: [v1](https://laion.ai/blog/open-flamingo/) | [v2](https://laion.ai/blog/open-flamingo-v2/)

model-zoo link: [here](https://github.com/mlfoundations/open_flamingo?tab=readme-ov-file#released-openflamingo-models)

citation: 
```bibtex
@article{awadalla2023openflamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}
```


#### MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.10592)

github link: [here](https://github.com/Vision-CAIR/MiniGPT-4)

homepage link: [here](https://minigpt-4.github.io/)

citation: 
```bibtex
@misc{zhu2023minigpt4,
      title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
      year={2023},
      eprint={2304.10592},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Visual instruction tuning (LLaVA) [`READ`]

paper link: [here](https://arxiv.org/pdf/2304.08485)

homepage link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)

model-zoo link: [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v1)

dataset-zoo link: [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/Data.md)

updated version: [here](#improved-baselines-with-visual-instruction-tuning-llava-v15-read)

citation: 
```bibtex
@inproceedings{liu2023llava,
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title = {Visual Instruction Tuning},
    booktitle = {NeurIPS},
    year = {2023}
}
```
    

#### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Q-Former) [`READ`]

paper link: [here](https://arxiv.org/pdf/2301.12597)

github link: [here](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)


citation:
```bibtex
@misc{li2023blip2,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}     
```


#### Visual programming: Compositional visual reasoning without training [`READ`]

paper link: [here](https://arxiv.org/pdf/2211.11559)

github link: [here](https://github.com/allenai/visprog)

homepage link: [here](https://prior.allenai.org/projects/visprog)

citation: 
```bibtex
@inproceedings{gupta2023visual,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}
```


#### Flamingo: a Visual Language Model for Few-Shot Learning [`READ`]

paper link: [here](https://arxiv.org/pdf/2204.14198)

blog link: [here](https://sh-tsang.medium.com/review-flamingo-a-visual-language-model-for-few-shot-learning-ec477d47e7bf)

github link: [here](https://github.com/lucidrains/flamingo-pytorch)

open-source variants: [idefics](#introducing-idefics-an-open-reproduction-of-state-of-the-art-visual-language-model-idefics-read) | [open-flamingo](#openflamingo-an-open-source-framework-for-training-large-autoregressive-vision-language-models-read)

citation: 
```bibtex
@misc{alayrac2022flamingo,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Learning transferable visual models from natural language supervision (CLIP) [`READ`]

paper link: [here](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)

github link: [here](https://github.com/OpenAI/CLIP)

citation: 
```bibtex
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
```
