# Finetuning Strategies for Multimodal Large Language Models
*Here're some resources about how to finetune MMLMs*

## Video Post Tuning

#### ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning

paper link: [here](https://arxiv.org/pdf/2406.14130v1)

github link: [here](https://github.com/modelscope/DiffSynth-Studio)

citation:

```bibtex
@misc{duan2024exvideoextendingvideodiffusion,
      title={ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning}, 
      author={Zhongjie Duan and Wenmeng Zhou and Cen Chen and Yaliang Li and Weining Qian},
      year={2024},
      eprint={2406.14130},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.14130}, 
}
```


## Visual Instruction Tuning


#### MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning

paper link: [here](https://arxiv.org/pdf/2311.10774)

github link: [here](https://github.com/FuxiaoLiu/MMC)

dataset link: [here](https://github.com/FuxiaoLiu/MMC?tab=readme-ov-file#mmc-instruction-dataset)


citation: 
```bibtex
@misc{liu2024mmc,
      title={MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning}, 
      author={Fuxiao Liu and Xiaoyang Wang and Wenlin Yao and Jianshu Chen and Kaiqiang Song and Sangwoo Cho and Yaser Yacoob and Dong Yu},
      year={2024},
      eprint={2311.10774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Improved Baselines with Visual Instruction Tuning (LLaVA-v1.5)

paper link: [here](https://arxiv.org/pdf/2310.03744.pdf)

blog link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)

model-zoo links: [v1.5](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v15) | [v1.6](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16)


citation: 
```bibtex
@misc{liu2023improvedllava,
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      title={Improved Baselines with Visual Instruction Tuning}, 
      publisher={arXiv:2310.03744},
      year={2023},
}
```

#### Visual instruction tuning (LLaVA)

paper link: [here](https://arxiv.org/pdf/2304.08485)

homepage link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)

model-zoo link: [here](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v1)

updated version: [here](https://arxiv.org/pdf/2310.03744.pdf)

citation: 
```bibtex
@inproceedings{liu2023llava,
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title = {Visual Instruction Tuning},
    booktitle = {NeurIPS},
    year = {2023}
}
```


#### OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework

paper link: [here](https://arxiv.org/pdf/2202.03052)

github link: [here](https://github.com/OFA-Sys/OFA)

model-zoo link: [here](https://github.com/OFA-Sys/OFA/blob/main/checkpoints.md)

citation:
```bibtex
@misc{wang2022ofa,
      title={OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework}, 
      author={Peng Wang and An Yang and Rui Men and Junyang Lin and Shuai Bai and Zhikang Li and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},
      year={2022},
      eprint={2202.03052},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```