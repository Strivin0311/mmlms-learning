# Evaluation Benchmarks for Multimodal Large Language Models
*Here're some resources about evaluation benchmarks for mmlms to evaluate its multimodal tassks performance*


### Video Multi-Tasks


#### Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.16103)

github link: [here](https://github.com/PKU-YuanGroup/Video-Bench)

dataset link: [here](https://huggingface.co/datasets/LanguageBind/Video-Bench)


citation:
```bibtex
@misc{ning2023videobench,
      title={Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models}, 
      author={Munan Ning and Bin Zhu and Yujia Xie and Bin Lin and Jiaxi Cui and Lu Yuan and Dongdong Chen and Li Yuan},
      year={2023},
      eprint={2311.16103},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


### Visual Multi-Tasks


#### SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2404.16790)

github link: [here](https://github.com/AILab-CVC/SEED-Bench)

dataset link: [here](https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2-plus)


citation:
```bibtex
@misc{li2024seedbench2plus,
      title={SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension}, 
      author={Bohao Li and Yuying Ge and Yi Chen and Yixiao Ge and Ruimao Zhang and Ying Shan},
      year={2024},
      eprint={2404.16790},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Are We on the Right Way for Evaluating Large Vision-Language Models? (MMStar) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2403.20330)

github link: [here](https://github.com/MMStar-Benchmark/MMStar)

homepage link: [here](https://mmstar-benchmark.github.io/)

dataset link: [here](https://huggingface.co/datasets/Lin-Chen/MMStar)


citation:
```bibtex   
@misc{chen2024right,
      title={Are We on the Right Way for Evaluating Large Vision-Language Models?}, 
      author={Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Jiaqi Wang and Yu Qiao and Dahua Lin and Feng Zhao},
      year={2024},
      eprint={2403.20330},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs (Q-Bench+) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2402.07116)

github link: [here](https://github.com/Q-Future/Q-Bench)

dataset link: [here](https://huggingface.co/datasets/q-future/q-bench2)


citation:
```bibtex
@misc{zhang2024benchmark,
      title={A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs}, 
      author={Zicheng Zhang and Haoning Wu and Erli Zhang and Guangtao Zhai and Weisi Lin},
      year={2024},
      eprint={2402.07116},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### SEED-Bench-2: Benchmarking Multimodal Large Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2311.17092)

github link: [here](https://github.com/AILab-CVC/SEED-Bench)

dataset link: [here](https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2)

updated version: [here](#seed-bench-2-plus-benchmarking-multimodal-large-language-models-with-text-rich-visual-comprehension-unread)

citation:
```bibtex
@misc{li2023seedbench2,
      title={SEED-Bench-2: Benchmarking Multimodal Large Language Models}, 
      author={Bohao Li and Yuying Ge and Yixiao Ge and Guangzhi Wang and Rui Wang and Ruimao Zhang and Ying Shan},
      year={2023},
      eprint={2311.17092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.14181)

github link: [here](https://github.com/Q-Future/Q-Bench)

dataset link: [LLVisionQA](https://huggingface.co/datasets/teowu/LLVisionQA-QBench) | [LLDescribe](https://huggingface.co/datasets/teowu/LLDescribe-QBench)

updated version: [here](#a-benchmark-for-multi-modal-foundation-models-on-low-level-vision-from-single-images-to-pairs-q-bench-unread)

citation:
```bibtex
@misc{wu2024qbench,
      title={Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision}, 
      author={Haoning Wu and Zicheng Zhang and Erli Zhang and Chaofeng Chen and Liang Liao and Annan Wang and Chunyi Li and Wenxiu Sun and Qiong Yan and Guangtao Zhai and Weisi Lin},
      year={2024},
      eprint={2309.14181},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2307.16125)

github link: [here](https://github.com/AILab-CVC/SEED-Bench)

dataset link: [here](https://huggingface.co/datasets/AILab-CVC/SEED-Bench)

updated version: [here](#seed-bench-2-benchmarking-multimodal-large-language-models-unread)

citation:
```bibtex
@misc{li2023seedbench,
      title={SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension}, 
      author={Bohao Li and Rui Wang and Guangzhi Wang and Yuying Ge and Yixiao Ge and Ying Shan},
      year={2023},
      eprint={2307.16125},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### MMBench: Is Your Multi-modal Model an All-around Player? [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2307.06281)

github link: [here](https://github.com/open-compass/VLMEvalKit)

dataset link: [here](https://github.com/open-compass/mmbench/)

citation:
```bibtex
@misc{liu2024mmbench,
      title={MMBench: Is Your Multi-modal Model an All-around Player?}, 
      author={Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
      year={2024},
      eprint={2307.06281},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.13394)

github link: [here](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)

dataset link: [here](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation?tab=readme-ov-file)

citation: 
```bibtex
@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and Wu, Yunsheng and Ji, Rongrong},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}
```





### Visual Question Answering (VQA)


#### MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2308.02490)

github link: [here](https://github.com/yuweihao/MM-Vet)

dataset link: [here](https://github.com/yuweihao/MM-Vet?tab=readme-ov-file#evalute-your-model-on-mm-vet)

citation:
```bibtex
@inproceedings{yu2024mm,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  booktitle={International conference on machine learning},
  year={2024},
  organization={PMLR}
}
```

#### Visual instruction tuning (LLaVA-Bench) [`READ`]

paper link: [here](https://arxiv.org/pdf/2304.08485)

homepage link: [here](https://llava-vl.github.io/)

github link: [here](https://github.com/haotian-liu/LLaVA)

dataset link: [in-the-wild](https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild) | [coco](https://huggingface.co/datasets/lmms-lab/llava-bench-coco)

citation: 
```bibtex
@inproceedings{liu2023llava,
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title = {Visual Instruction Tuning},
    booktitle = {NeurIPS},
    year = {2023}
}
```



#### Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2209.09513)

github link: [here](https://github.com/lupantech/ScienceQA)

homepage link: [here](https://scienceqa.github.io/)

dataset link: [here](https://scienceqa.github.io/#dataset)

citation:
```bibtex
@misc{lu2022learn,
      title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, 
      author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
      year={2022},
      eprint={2209.09513},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### VizWiz Grand Challenge: Answering Visual Questions from Blind People (VizWiz) [`UNREAD`]

paper link: [here](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.pdf)

homepage link: [here](https://vizwiz.org/tasks-and-datasets/vqa/)

dataset link: [here](https://vizwiz.cs.colorado.edu/VizWiz_final/images/)

citation:
```bibtex
@misc{gurari2018vizwiz,
      title={VizWiz Grand Challenge: Answering Visual Questions from Blind People}, 
      author={Danna Gurari and Qing Li and Abigale J. Stangl and Anhong Guo and Chi Lin and Kristen Grauman and Jiebo Luo and Jeffrey P. Bigham},
      year={2018},
      eprint={1802.08218},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering (VQA) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1612.00837)

github link: [here](https://github.com/jiasenlu/HieCoAttenVQA)

homepage link: [here](https://visualqa.org/)

dataset link: [here](https://visualqa.org/download.html)

citation:
```bibtex
@misc{goyal2017making,
      title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
      author={Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},
      year={2017},
      eprint={1612.00837},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


### Hallucination Probing


#### HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2310.14566)

github link: [here](https://github.com/tianyi-lab/HallusionBench)

dataset link: [here](https://github.com/tianyi-lab/HallusionBench?tab=readme-ov-file#dataset-download)

citation:
```bibtex
@misc{guan2024hallusionbench,
      title={HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}, 
      author={Tianrui Guan and Fuxiao Liu and Xiyang Wu and Ruiqi Xian and Zongxia Li and Xiaoyu Liu and Xijun Wang and Lichang Chen and Furong Huang and Yaser Yacoob and Dinesh Manocha and Tianyi Zhou},
      year={2024},
      eprint={2310.14566},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


#### POPE: Polling-based Object Probing Evaluation for Object Hallucination (POPE) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.10355)

github link: [here](https://github.com/AoiDragon/POPE)

dataset link: [here](https://github.com/AoiDragon/POPE?tab=readme-ov-file#build-pope)

citation:
```bibtex
@inproceedings{Li-hallucination-2023,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao and Ji-Rong Wen},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://openreview.net/forum?id=xozJw0kZXF}
}
```